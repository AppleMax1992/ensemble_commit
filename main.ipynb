{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f82a8cc-7b02-48d7-a783-6e89bccff8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T01:17:38.159980Z",
     "iopub.status.busy": "2024-06-19T01:17:38.158978Z",
     "iopub.status.idle": "2024-06-19T01:17:38.169490Z",
     "shell.execute_reply": "2024-06-19T01:17:38.167651Z",
     "shell.execute_reply.started": "2024-06-19T01:17:38.159900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ensemble_model.preprocesser as preprocesser \n",
    "import ensemble_model.ensemble_model-backuo as em \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581b2000-7c82-4109-9761-1f0ea8e3f727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T01:12:48.406796Z",
     "iopub.status.busy": "2024-06-19T01:12:48.405773Z",
     "iopub.status.idle": "2024-06-19T01:12:53.189128Z",
     "shell.execute_reply": "2024-06-19T01:12:53.187407Z",
     "shell.execute_reply.started": "2024-06-19T01:12:48.406715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1340/2618084632.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"label\": label2id})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(r'/root/autodl-tmp/CommitFit/dataset/dataset.csv', index_col=0, encoding='utf_8_sig')\n",
    "df.fillna('', inplace=True)\n",
    "label2id={'negative':0,'positive':1}\n",
    "df = df.replace({\"label\": label2id})\n",
    "df\n",
    "df['command'] = df['diff'].apply(lambda x : ' '.join(x.split('@@')[:2]))\n",
    "df\n",
    "df_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "  df_dataset.append([row['message'],row['command'],row['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b77e6e8-8d9d-45f3-be04-94a8b7499213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T01:12:53.192638Z",
     "iopub.status.busy": "2024-06-19T01:12:53.192328Z",
     "iopub.status.idle": "2024-06-19T01:12:56.353770Z",
     "shell.execute_reply": "2024-06-19T01:12:56.352581Z",
     "shell.execute_reply.started": "2024-06-19T01:12:53.192609Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/models/bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT and CodeBERT models and tokenizers\n",
    "bert_model = BertModel.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "\n",
    "codebert_model = RobertaModel.from_pretrained('/root/autodl-tmp/models/codebert-base')\n",
    "codebert_tokenizer = RobertaTokenizer.from_pretrained('/root/autodl-tmp/models/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1549be02-6f6b-41e5-bd04-d3e164061eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T09:40:31.472708Z",
     "iopub.status.busy": "2024-06-19T09:40:31.470744Z",
     "iopub.status.idle": "2024-06-19T10:03:42.815597Z",
     "shell.execute_reply": "2024-06-19T10:03:42.813843Z",
     "shell.execute_reply.started": "2024-06-19T09:40:31.472598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 0.0990113877590558\n",
      "Epoch 2/12, Loss: 0.05699514417877796\n",
      "Epoch 3/12, Loss: 0.04168494194877157\n",
      "Epoch 4/12, Loss: 0.030777447279372434\n",
      "Epoch 5/12, Loss: 0.028709152421862047\n",
      "Epoch 6/12, Loss: 0.022664402571981664\n",
      "Epoch 7/12, Loss: 0.02724408796670241\n",
      "Epoch 8/12, Loss: 0.026725432173752277\n",
      "Epoch 9/12, Loss: 0.017468110373732703\n",
      "Epoch 10/12, Loss: 0.02127898242984316\n",
      "Epoch 11/12, Loss: 0.020804244124555702\n",
      "Epoch 12/12, Loss: 0.02167102491943577\n",
      "Validation Accuracy: 0.9668808699950568\n",
      "Precision: 0.9528061224489796\n",
      "Recall: 0.9613899613899614\n",
      "F1-Score: 0.957078795643818\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "# preprocesser = preprocesser.SentencePairDataset(train_data, bert_tokenizer, codebert_tokenizer)\n",
    "train_data, val_data = train_test_split(df_dataset, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = preprocesser.SentencePairDataset(train_data, bert_tokenizer, codebert_tokenizer)\n",
    "val_dataset = preprocesser.SentencePairDataset(val_data, bert_tokenizer, codebert_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the model\n",
    "# model = em.CombinedModel(bert_model, codebert_model, bert_tokenizer, codebert_tokenizer)\n",
    "\n",
    "# # Train the model\n",
    "# model.trainer(train_loader, val_loader,num_epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "base_model1 = em.BaseModel(bert_model,bert_tokenizer)\n",
    "base_model2 = em.BaseModel(codebert_model,codebert_tokenizer)\n",
    "\n",
    "# Create stacking model\n",
    "stacking_model = em.StackingModel(base_model1, base_model2)\n",
    "# Train the model\n",
    "stacking_model.trainer(train_loader, val_loader,num_epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de47b49-8d23-4eca-9690-1cff4771159b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35729cd-f338-4b5f-93ad-088419a0c2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
