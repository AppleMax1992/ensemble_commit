{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81404e-4d72-4b9a-9fe5-e4d35ba46fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define base model class\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.transformer_model = transformer_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.transformer_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs\n",
    "\n",
    "# Define stacking model class\n",
    "class StackingModel(nn.Module):\n",
    "    def __init__(self, base_model1, base_model2):\n",
    "        super(StackingModel, self).__init__()\n",
    "        self.base_model1 = base_model1\n",
    "        self.base_model2 = base_model2\n",
    "        self.classifier = nn.Linear(base_model1.transformer_model.config.hidden_size + base_model2.transformer_model.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, inputs_bert, inputs_codebert):\n",
    "        outputs_bert = self.base_model1(inputs_bert)\n",
    "        outputs_codebert = self.base_model2(inputs_codebert)\n",
    "        combined = torch.cat((outputs_bert, outputs_codebert), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "    def predict(self, sentence1, sentence2):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs_bert = self.base_model1.transformer_model(**sentence1).last_hidden_state[:, 0, :]\n",
    "            inputs_codebert = self.base_model2.transformer_model(**sentence2).last_hidden_state[:, 0, :]\n",
    "            combined = torch.cat((inputs_bert, inputs_codebert), dim=1)\n",
    "            logits = self.classifier(combined)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            return predicted_class, probabilities.cpu().numpy()\n",
    "\n",
    "    def trainer(self, train_loader, val_loader, num_epochs=3, learning_rate=2e-5):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                inputs_bert, inputs_codebert, labels = batch\n",
    "                inputs_bert = inputs_bert.to(device)\n",
    "                inputs_codebert = inputs_codebert.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self(inputs_bert, inputs_codebert)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
    "\n",
    "        self.evaluate(val_loader)\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs_bert, inputs_codebert, labels = batch\n",
    "                inputs_bert = inputs_bert.to(device)\n",
    "                inputs_codebert = inputs_codebert.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = self(inputs_bert, inputs_codebert)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = correct / total\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "        print(f'Validation Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1-Score: {f1}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize BERT and CodeBERT models and tokenizers\n",
    "    bert_model = BertModel.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "    codebert_model = RobertaModel.from_pretrained('/root/autodl-tmp/models/codebert-base')\n",
    "    codebert_tokenizer = RobertaTokenizer.from_pretrained('/root/autodl-tmp/models/codebert-base')\n",
    "    # # Load BERT and CodeBERT models and tokenizers\n",
    "    # bert_model = BertModel.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "    # bert_tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/models/bert-base-cased')\n",
    "    \n",
    "    # codebert_model = RobertaModel.from_pretrained('/root/autodl-tmp/models/codebert-base')\n",
    "    # codebert_tokenizer = RobertaTokenizer.from_pretrained('/root/autodl-tmp/models/codebert-base')\n",
    "    # Create base models\n",
    "    base_model1 = BaseModel(bert_model)\n",
    "    base_model2 = BaseModel(codebert_model)\n",
    "\n",
    "    # Create stacking model\n",
    "    stacking_model = StackingModel(base_model1, base_model2)\n",
    "\n",
    "    # Example training and evaluation\n",
    "    # Replace train_loader and val_loader with your actual data loaders\n",
    "    train_loader = []  # Replace with your training data loader\n",
    "    val_loader = []    # Replace with your validation data loader\n",
    "    stacking_model.trainer(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d43db-8951-4f18-957b-2ed386b56bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
